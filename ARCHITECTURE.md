# CognitionSim SPI Architecture

## System Overview

CognitionSim implements a **Stateful Symbolic Predictive Interpreter (SPI)** that combines:

- **Symbolic reasoning** via pattern algebra and logical inference
- **Spiking neural dynamics** with threshold-based activation
- **Neuroplastic adaptation** across requests
- **Temporal oscillations** for phase-modulated responses
- **Governance-aware inference** with policy-conditioned output gating

This is fundamentally incompatible with stateless commodity ML. It's an **agent architecture**, not a prediction service.

> ðŸ’¡ **Understanding the Language:** This document uses both poetic metaphors and mechanical specifications. For a complete dual-language reference that translates concepts like "field breathing" into precise mathematical equations, see [DUAL_LANGUAGE_GLOSSARY.md](./DUAL_LANGUAGE_GLOSSARY.md).

---

## Core Components

### 1. **Symbolic Layer** (`quadra/core/symbolic/`)

#### `SymbolicConfig`
```python
@dataclass
class SymbolicConfig:
    input_dim: int = 128         # Input feature space
    hidden_dim: int = 256        # Internal representation
    output_dim: int = 64         # Symbolic output
    num_layers: int = 3          # Stacking depth
```

#### `PatternModule`
Extracts and reconstructs patterns via bottleneck:
```
Input (128) â†’ FC1 (256) â†’ ReLU â†’ FC2 (256) â†’ ReLU â†’ FC3 (128) â†’ Output
```
Acts as **pattern codec** for lossy compression. Sparse patterns emerge.

---

### 2. **Neural Dynamics Layer** (`quadra/core/neural/`)

#### `SpikingFieldUpdateNN` 
Implements spiking field equations with thresholds:
```
potential(x) = sigmoid(FC(x))
spike = (potential > threshold)
output = spike * potential  [multiply-by-spike gating]
```

This is **not** standard ReLU. The spike gate is sparse and temporal.

#### `OscillatorySynapseTheory`
Phase-modulated synaptic transmission:
```
phase(t) = phase(t-1) + 0.1
output = tanh(FC(x)) * sin(phase(t))
```

Creates **temporal periodicity** â€” output magnitude oscillates at fixed frequency. Essential for:
- Circadian-like patterns
- Resonance phenomena
- Energy conservation

#### `SpikingSyntropyNN`
"Negative entropy" or order-promoting layer:
```
output = sigmoid(FC(x))
```
Drives the system toward **organized, low-entropy states** during inference.

#### `NeuroplasticityManager`
Exponential memory with plasticity:
```
memory(t) = 0.9 * memory(t-1) + 0.1 * adapted(t)
adapted(t) = tanh(FC(x))
```

**Critical property**: Memory persists across requests. Enables learning from interaction history.

#### `SpikingFeedbackNN`
Implements **recurrent feedback** from output to hidden state:
```
hidden' = hidden + 0.1 * feedback
output = ReLU(FC(hidden'))
```

Enables **attractor dynamics** and error correction.

---

### 3. **Governance Layer** (`quadra/core/governance/`)

#### `PolicyAdapter`
Maps system state â†’ governance policy:
```python
class PolicyAdapter:
    def evaluate(self, context: InferenceContext) -> GovernancePolicy:
        """
        Given model state + input, determine:
        - Should output be suppressed?
        - Should explanation be attached?
        - Should prediction be deferred?
        """
        return policy
```

#### `GovernanceScorer`
Computes governance scores per output dimension:
```
score = policy_embedding @ output_representation
if score > threshold: output *= suppression_factor (e.g., 0.3)
```

---

### 4. **State Management** (`quadra/state/`)

#### `MemoryStore`
Persistent neuroplastic state:
```python
store.update_memory(session_id, new_state)
state = store.get_memory(session_id, age_decay=0.95)
```

Enables:
- **Cross-request learning** (memory persists)
- **Temporal decay** (older memories fade)
- **Session isolation** (per-user state)

---

## Data Flow: The Complete Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw Input (128) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Pattern Encoding  â”‚  PatternModule
â”‚   Input â†’ Bottleneck â”‚  Extracts sparse features
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Field Spiking     â”‚  SpikingFieldUpdateNN
â”‚   threshold gating   â”‚  Sparse temporal activation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Syntropy         â”‚  SpikingSyntropyNN
â”‚   Order-promoting    â”‚  Drive toward organized states
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Neuroplasticity   â”‚  NeuroplasticityManager
â”‚   Adaptive memory    â”‚  Update self.memory (EMA)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Oscillation       â”‚  OscillatorySynapseTheory
â”‚   Phase modulation   â”‚  Temporal sinusoidal gating
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Feedback Loop     â”‚  SpikingFeedbackNN
â”‚   Recurrent update   â”‚  Error correction, attractor
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Symbolic Reason   â”‚  SymbolicPredictiveInterpreter
â”‚   Pattern algebra    â”‚  Extract logical structure
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Governance Gate   â”‚  PolicyAdapter + GovernanceScorer
â”‚   Policy-conditioned â”‚  Suppress/explain as needed
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output (64 dims)     â”‚  Policy-gated result
â”‚ + Confidence         â”‚  + Explanation if required
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## State Persistence Architecture

### Session-Scoped State

```
Request 1: Input_1 â†’ [Inference Pipeline] â†’ NeuroplasticityManager.memory = M_1
                                           â†’ MemoryStore[session_id] = M_1

Request 2: Input_2 â†’ [Inference Pipeline] â†’ NeuroplasticityManager.memory = 0.9*M_1 + 0.1*adapted_2
                                           â†’ MemoryStore[session_id] = M_2
                     (Memory "remembers" Request 1's effect)

Request 3: Input_3 â†’ [Same pipeline] â†’ Memory blended from Requests 1 & 2
```

**Result**: Model **adapts** to session-specific patterns. Not stateless batch processing.

---

## Governance-Inference Integration

### Policy-Conditioned Output

```python
# After inference pipeline produces `output`:

policy = governance_adapter.evaluate(context)

# Gate 1: Suppression
if policy.requires_suppression:
    output *= 0.3  # Confidence reduction

# Gate 2: Explanation
if policy.requires_explanation:
    # Attach symbolic trace showing decision path
    output.explanation = symbolic_trace

# Gate 3: Escalation
if policy.requires_escalation:
    # Route to human reviewer
    defer_prediction(output)
    return None
```

**Key insight**: Governance is **not post-hoc moderation**. It's **embedded in inference**. The model learns that certain outputs get gated, shaping learned behavior.

---

## Comparison: Commodity ML vs. SPI Architecture

| Aspect | Commodity ML | SPI |
|--------|-------------|-----|
| **State** | Stateless batch processing | Persistent memory per session |
| **Memory** | Input-output pairs | Internal neuroplastic state |
| **Governance** | Post-hoc filtering | Policy-conditioned inference |
| **Adaptation** | Requires retraining | Online learning via memory |
| **Temporal** | None (iid assumption) | Phase-modulated, oscillatory |
| **Feedback** | Supervised labels | Self-correcting recurrent loops |
| **Architecture** | Feed-forward | Recurrent + memory + governance |

---

## Deployment Topology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Flask API Gateway            â”‚
â”‚  (TLS, Auth, Request Routing)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Inference Service (replicated)     â”‚
â”‚  â€¢ SymbolicPredictiveInterpreter       â”‚
â”‚  â€¢ PolicyAdapter                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stateful Memory Store (redis/db)     â”‚
â”‚  â€¢ MemoryStore[session_id] â†’ state     â”‚
â”‚  â€¢ TTL-based expiry                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Governance Policy Engine             â”‚
â”‚  â€¢ PolicyAdapter                       â”‚
â”‚  â€¢ SocialGovernanceService             â”‚
â”‚  â€¢ Audit log (all decisions)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Critical Design Decisions

### 1. **Multiplicative Gating, Not Additive**
- Output *= gating_factor (preserves structure)
- Not output += correction (can destroy learned patterns)

### 2. **Exponential Memory Decay**
- memory(t) = 0.9 * memory(t-1) + 0.1 * new
- Forgets old patterns gracefully
- Not hard reset (preserves adaptation)

### 3. **Spiking > ReLU**
- Spike gate: (potential > threshold).float()
- ReLU: max(0, x)
- Spikes are **sparse, temporal, meaningful**

### 4. **Policy as First-Class**
- Not filtering output_post_inference
- Policy shapes network behavior during forward pass
- Enables **learned compliance**

---

## Next Steps for Production

1. **Load persistent state on startup** â†’ MemoryStore from checkpoint
2. **Governance policy versioning** â†’ Track policy changes for audit
3. **Monitoring metrics** â†’ Track memory staleness, policy enforcement rate
4. **Failure recovery** â†’ If MemoryStore unavailable, degrade to stateless mode
5. **Multi-agent communication** â†’ Oscillatory synchronization between instances
