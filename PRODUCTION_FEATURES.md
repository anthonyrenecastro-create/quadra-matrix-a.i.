# Production Features Implementation Guide

## Overview

This document describes the advanced production features that have been implemented for Quadra Matrix A.I. All required modules and configurations are ready to use.

---

## ‚úÖ 1. Prometheus Metrics

### Implementation Status: COMPLETE

**Module**: `utils/metrics.py`

### Features:
- HTTP request metrics (requests/second, latency, status codes)
- WebSocket metrics (connections, messages)
- Training metrics (loss, reward, variance, duration)
- Model inference metrics (latency per version)
- Database operation metrics
- Error tracking
- Custom decorators for automatic instrumentation

### Usage:

```python
# In your app.py, add:
from utils.metrics import metrics_manager, track_request_metrics, update_training_metrics
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST

# Initialize metrics
metrics_manager.initialize('1.0.0', 'production')

# Add metrics endpoint
@app.route('/metrics')
def metrics():
    return Response(generate_latest(), mimetype=CONTENT_TYPE_LATEST)

# Track requests automatically
@app.route('/api/endpoint')
@track_request_metrics
def my_endpoint():
    return jsonify({'status': 'ok'})

# Update training metrics
update_training_metrics(loss=0.05, reward=0.95, variance=0.02)
```

### Metrics Endpoint:
- **URL**: `http://localhost:5000/metrics`
- **Format**: Prometheus exposition format
- **Integration**: Configure Prometheus to scrape this endpoint

### Prometheus Configuration:

Add to `prometheus.yml`:
```yaml
scrape_configs:
  - job_name: 'quadra-matrix'
    static_configs:
      - targets: ['localhost:5000']
    metrics_path: '/metrics'
    scrape_interval: 15s
```

---

## ‚úÖ 2. Structured Logging for ELK/Loki

### Implementation Status: COMPLETE

**Module**: `utils/structured_logging.py`

### Features:
- JSON-formatted logs
- Integration configurations for Logstash, Promtail, Fluentd
- Docker Compose templates for ELK and Loki stacks
- Automatic metadata enrichment (timestamp, level, module, function, line)

### Usage:

```python
# In config.py, add:
USE_STRUCTURED_LOGGING = os.getenv('USE_STRUCTURED_LOGGING', 'false').lower() == 'true'

# In app.py:
from utils.structured_logging import setup_structured_logging

if config.USE_STRUCTURED_LOGGING:
    setup_structured_logging(
        log_level='INFO',
        logs_dir=Path('logs'),
        enable_console=True,
        enable_file=True
    )
```

### Log Files:
- `logs/quadra_matrix.json.log` - All logs in JSON format
- `logs/quadra_matrix_error.json.log` - Errors only

### ELK Stack Setup:

```bash
# Copy Logstash configuration
python -c "from utils.structured_logging import save_log_aggregation_configs; from pathlib import Path; save_log_aggregation_configs(Path('logstash'))"

# Start ELK stack (Docker Compose configuration provided in module)
docker-compose -f docker-compose-elk.yml up -d
```

### Grafana Loki Setup:

```bash
# Start Loki stack
docker-compose -f docker-compose-loki.yml up -d

# Access Grafana at http://localhost:3000
# Default credentials: admin/admin
```

---

## ‚úÖ 3. Redis for SocketIO Scaling

### Implementation Status: COMPLETE

**Module**: `utils/redis_config.py`

### Features:
- Redis connection pooling
- Redis Sentinel support for high availability
- Automatic failover
- Health checking
- SocketIO message queue configuration

### Usage:

```python
# In config.py, add:
REDIS_ENABLED = os.getenv('REDIS_ENABLED', 'false').lower() == 'true'
REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))

# In app.py:
from utils.redis_config import RedisConfig

if config.REDIS_ENABLED:
    redis_config = RedisConfig()
    socketio_config = redis_config.get_socketio_config()
    socketio = SocketIO(app, **socketio_config)
```

### Environment Variables:

Add to `.env`:
```bash
REDIS_ENABLED=true
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=your-secure-password
REDIS_DB=0
REDIS_MAX_CONNECTIONS=50
```

### Docker Compose:

```yaml
# Add to docker-compose.yml
services:
  redis:
    image: redis:7.2-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s

volumes:
  redis-data:
```

---

## ‚úÖ 4. Rate Limiting

### Implementation Status: COMPLETE

**Dependencies**: Flask-Limiter + Redis (optional)

### Usage:

```python
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

# Initialize with Redis backend
limiter = Limiter(
    app=app,
    key_func=get_remote_address,
    storage_uri='redis://localhost:6379',
    default_limits=["200 per day", "50 per hour"]
)

# Apply to specific endpoints
@app.route('/api/endpoint')
@limiter.limit("10 per minute")
def rate_limited_endpoint():
    return jsonify({'status': 'ok'})
```

### Rate Limit Configuration:

```python
# Global limits
default_limits=["200 per day", "50 per hour"]

# Per-endpoint limits
@limiter.limit("100 per hour")  # Training endpoints
@limiter.limit("1000 per hour")  # Read endpoints
@limiter.limit("10 per minute")  # Heavy operations
```

---

## ‚úÖ 5. Database Migrations (Alembic)

### Implementation Status: COMPLETE

**Files**:
- `alembic.ini` - Alembic configuration
- `alembic/env.py` - Migration environment
- `alembic/script.py.mako` - Migration template
- `alembic/versions/` - Migration files

### Usage:

```bash
# Initialize (already done)
alembic init alembic

# Create a new migration
alembic revision --autogenerate -m "Add new column"

# Apply migrations
alembic upgrade head

# Rollback last migration
alembic downgrade -1

# Show current version
alembic current

# Show migration history
alembic history
```

### Auto-generate Migrations:

```bash
# After changing database.py models:
alembic revision --autogenerate -m "Updated SystemState model"
alembic upgrade head
```

### Environment Variables:

```bash
# Override database URL
export DATABASE_URL="postgresql://user:pass@localhost/quadra_matrix"
alembic upgrade head
```

---

## ‚úÖ 6. Performance Benchmarks

### Implementation Status: COMPLETE

**Module**: `utils/benchmarks.py`  
**Script**: `scripts/run_benchmarks.py`

### Features:
- Benchmark suite framework
- Statistical analysis (mean, median, p95, p99)
- JSON result export
- Continuous performance monitoring
- Specific benchmarks for ML operations, database, API endpoints

### Usage:

```bash
# Run benchmarks
python scripts/run_benchmarks.py

# Run with custom iterations
python scripts/run_benchmarks.py --iterations 500

# Run with custom output
python scripts/run_benchmarks.py --output benchmarks/perf_2025.json

# Run verbose mode
python scripts/run_benchmarks.py --verbose
```

### Custom Benchmarks:

```python
from utils.benchmarks import BenchmarkSuite

suite = BenchmarkSuite('My Operations')

# Method 1: Use run_benchmark
def my_operation():
    # code to benchmark
    pass

suite.run_benchmark('operation_name', my_operation, iterations=100)

# Method 2: Use measure context
benchmark = suite.create_benchmark('manual_operation')
for i in range(100):
    with benchmark.measure():
        # code to benchmark
        pass

# Print results
suite.print_summary()

# Save results
suite.save_results(Path('results.json'))
```

### Continuous Monitoring:

```python
from utils.benchmarks import performance_monitor

# Track operations
with performance_monitor.track('database_query'):
    db.query_something()

# Save metrics periodically
performance_monitor.save_metrics()
```

---

## ‚úÖ 7. Automated Backups

### Implementation Status: COMPLETE

**Scripts**:
- `scripts/backup_models.sh` - Hourly model backups
- `scripts/full_backup.sh` - Complete system backup
- `scripts/setup_automated_backups.sh` - Cron configuration

### Setup:

```bash
# Make scripts executable
chmod +x scripts/*.sh

# Setup automated backups
bash scripts/setup_automated_backups.sh
```

### Backup Schedule:

```
Hourly   (0 * * * *) - Model artifacts
Daily    (0 2 * * *) - Database backup
Weekly   (0 3 * * 0) - Full system backup
Daily    (0 4 * * *) - Cleanup old backups (30 days retention)
```

### Manual Backups:

```bash
# Backup models
./scripts/backup_models.sh

# Full system backup
./scripts/full_backup.sh

# Verify backup
tar -tzf backups/models_backup_20251221_120000.tar.gz
sha256sum -c backups/models_backup_20251221_120000.tar.gz.sha256
```

### Backup Locations:

```
/app/backups/
‚îú‚îÄ‚îÄ models_backup_YYYYMMDD_HHMMSS.tar.gz
‚îú‚îÄ‚îÄ models_backup_YYYYMMDD_HHMMSS.tar.gz.sha256
‚îú‚îÄ‚îÄ db_backup_YYYYMMDD.db
‚îî‚îÄ‚îÄ full_backup_YYYYMMDD_HHMMSS.tar.gz
```

### Restore Procedures:

See [DISASTER_RECOVERY.md](DISASTER_RECOVERY.md) for complete restore procedures.

---

## üìä Monitoring Dashboard Setup

### Grafana Dashboard

```bash
# Start Grafana
docker run -d -p 3000:3000 grafana/grafana

# Access: http://localhost:3000
# Default: admin/admin

# Add Prometheus data source:
# Configuration > Data Sources > Add > Prometheus
# URL: http://prometheus:9090

# Add Loki data source:
# Configuration > Data Sources > Add > Loki
# URL: http://loki:3100
```

### Import Dashboard:

Use Grafana dashboard ID: 1860 (Node Exporter Full) or create custom dashboard with Quadra Matrix metrics.

---

## üöÄ Complete Docker Compose

Combine all services:

```yaml
version: '3.8'

services:
  quadra-matrix:
    build: .
    ports:
      - "5000:5000"
    environment:
      - REDIS_ENABLED=true
      - REDIS_HOST=redis
      - USE_STRUCTURED_LOGGING=true
      - DATABASE_URL=sqlite:///quadra_matrix.db
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
      - ./backups:/app/backups
    depends_on:
      - redis
      - prometheus

  redis:
    image: redis:7.2-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus

  loki:
    image: grafana/loki
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml

  promtail:
    image: grafana/promtail
    volumes:
      - ./logs:/app/logs:ro
      - ./promtail-config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki

volumes:
  redis-data:
  prometheus-data:
  grafana-data:
```

---

## üìù Configuration Summary

### Required Environment Variables:

```bash
# Core
SECRET_KEY=<generated-64-char-key>
FLASK_ENV=production
DATABASE_URL=sqlite:///quadra_matrix.db

# Redis
REDIS_ENABLED=true
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=<secure-password>

# Logging
USE_STRUCTURED_LOGGING=true
LOG_LEVEL=INFO

# CORS
CORS_ORIGINS=https://yourdomain.com
```

### Updated requirements.txt:

All required dependencies already added:
- `prometheus-client==0.19.0`
- `prometheus-flask-exporter==0.23.0`
- `Flask-Limiter==3.5.0`
- `redis==5.0.1`
- `python-json-logger==2.0.7`
- `alembic==1.13.1`

---

## ‚úÖ Verification Checklist

- [x] Prometheus metrics module created
- [x] Structured logging configured
- [x] Redis integration implemented
- [x] Rate limiting ready
- [x] Alembic migrations initialized
- [x] Benchmark suite created
- [x] Automated backups configured
- [x] Documentation complete

---

## üéØ Next Steps

1. **Enable Features**: Update `.env` file with required variables
2. **Start Services**: `docker-compose up -d`
3. **Verify Metrics**: Visit `http://localhost:5000/metrics`
4. **Check Monitoring**: Visit Grafana at `http://localhost:3000`
5. **Run Benchmarks**: `python scripts/run_benchmarks.py`
6. **Test Backups**: `./scripts/backup_models.sh`
7. **Run Migrations**: `alembic upgrade head`

---

**Implementation Date**: December 21, 2025  
**Status**: ‚úÖ ALL FEATURES COMPLETE AND READY TO USE
